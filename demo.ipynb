{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync Colab environment with pyproject.toml dependencies\n",
    "\n",
    "# 1. Clone your repo (change URL + folder name!)\n",
    "!git clone https://github.com/kziliask/drl_inventory_env.git\n",
    "%cd drl_inventory_env\n",
    "\n",
    "# 2. Install a TOML parser (tomli works well)\n",
    "!pip install tomli\n",
    "\n",
    "import tomli\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# 3. Load dependencies from [project] table\n",
    "pyproject_path = Path(\"pyproject.toml\")\n",
    "pyproject = tomli.loads(pyproject_path.read_text())\n",
    "\n",
    "deps = pyproject.get(\"project\", {}).get(\"dependencies\", [])\n",
    "\n",
    "# (Optional) If you don't want dev tools like mypy/pre-commit in Colab:\n",
    "deps = [d for d in deps if not d.startswith((\"mypy\", \"pre-commit\"))]\n",
    "\n",
    "print(\"Dependencies from pyproject.toml:\")\n",
    "for d in deps:\n",
    "    print(\"  -\", d)\n",
    "\n",
    "if deps:\n",
    "    print(\"\\nInstalling dependencies...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *deps])\n",
    "else:\n",
    "    print(\"No dependencies field found under [project].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_inventory_levels(env, model):\n",
    "    inventory_levels = []\n",
    "    rewards = []\n",
    "    obs, info = env.reset(seed=42)\n",
    "    done = False\n",
    "    for _ in range(100):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        inventory_levels.append(obs[0])\n",
    "        rewards.append(reward)\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            obs, info = env.reset()\n",
    "    inventory_levels = np.array(inventory_levels)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.step(range(len(inventory_levels)), inventory_levels, where=\"mid\")\n",
    "    plt.fill_between(\n",
    "        range(len(inventory_levels)), 0, inventory_levels, step=\"mid\", alpha=0.2\n",
    "    )\n",
    "    neg_indices = [i for i, r in enumerate(rewards) if r < 0]\n",
    "    plt.scatter(\n",
    "        neg_indices,\n",
    "        inventory_levels[neg_indices],\n",
    "        marker=\"o\",\n",
    "        color=\"red\",\n",
    "        label=\"Negative Reward\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Inventory Level\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class MinimalInventoryEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.demand_dist = [0, 4]  # uniform demand between 0 and 3\n",
    "        self.max_steps = 100\n",
    "        self.max_inventory = 10\n",
    "        # 1D state: inventory level x_t âˆˆ [0, 10]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0], dtype=np.float32),\n",
    "            high=np.array([self.max_inventory], dtype=np.float32),\n",
    "        )\n",
    "        # Discrete actions: order quantity {0, 1, 2, 3, 4, 5}\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.inv = 0.0\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Set initial inventory\n",
    "        self.step_count = 0\n",
    "        self.inv = self.np_random.integers(0, self.max_inventory + 1)\n",
    "        obs = np.array([self.inv], dtype=np.float32)\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        reward = 0.0\n",
    "        noise = self.np_random.integers(-1, 2)  # -1, 0, or +1 noise\n",
    "        order_qty = np.clip(int(action) + noise, 0, 5)\n",
    "        if self.inv + order_qty > self.max_inventory:\n",
    "            order_qty = self.max_inventory - self.inv\n",
    "            reward -= 2.0  # overstock penalty = stockout penalty\n",
    "        self.inv += order_qty\n",
    "        low, high = self.demand_dist\n",
    "        demand = self.np_random.integers(low, high)\n",
    "        demand_spike = self.np_random.random() < 0.2  # demand spikes\n",
    "        if demand_spike:\n",
    "            demand += 2\n",
    "        if demand > self.inv:\n",
    "            demand = self.inv  # lost sales\n",
    "            reward -= 1.0\n",
    "        else:\n",
    "            reward += 1.0\n",
    "        self.inv -= demand\n",
    "        terminated = self.step_count >= self.max_steps\n",
    "        truncated = False\n",
    "        obs = np.array([self.inv], dtype=np.float32)\n",
    "        info = {}\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be214fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MinimalInventoryEnv()\n",
    "check_env(env, warn=True)  # will print warnings if something is off\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=10_000, progress_bar=True)\n",
    "# Save and load your trained model\n",
    "# model.save(\"ppo_inventory\")\n",
    "# model = PPO.load(\"ppo_inventory\", env=env)\n",
    "\n",
    "plot_inventory_levels(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_kwargs = dict(\n",
    "#     net_arch=dict(\n",
    "#         pi=[64, 64],      # policy network\n",
    "#         vf=[64, 64]       # value function network\n",
    "#     ))\n",
    "# policy_kwargs = dict(net_arch=[64, 64])\n",
    "vec_env = make_vec_env(MinimalInventoryEnv, n_envs=4)\n",
    "%load_ext tensorboard\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", vec_env, verbose=0, tensorboard_log=\"./ppo_tensorboard/\", seed=42\n",
    ")\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./ppo_tensorboard/ --port 6006\n",
    "\n",
    "model.learn(total_timesteps=100_000, progress_bar=False)\n",
    "\n",
    "plot_inventory_levels(env, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
